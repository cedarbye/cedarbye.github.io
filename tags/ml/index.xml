<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on cedar&#39;s blog</title>
    <link>https://cedarbye.github.io/tags/ml/</link>
    <description>Recent content in Ml on cedar&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 24 Mar 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://cedarbye.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>分词算法——mmseg</title>
      <link>https://cedarbye.github.io/post/2016/2016_03_24%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95_mmseg/</link>
      <pubDate>Thu, 24 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2016/2016_03_24%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95_mmseg/</guid>
      <description>

&lt;p&gt;最大匹配算法的准确度似乎不高，啥正向最大匹配啊，说白了，就是拿着你的字典然后对文本进行暴力切割，这种算法切割出来的错误率肯定超级高，算法原理满大街都是，就不说了，时间复杂度倒挺低O(n)。&lt;/p&gt;

&lt;p&gt;接着就换算法，学习了个也比较简单的mmseg算法，这是台湾同胞提出来的，其实就是对正向最大匹配算法进行了优化，这里的优化不是指优化时间复杂度，是优化降低错误率的，但是也会出现很奇葩的错误，让人看了都蛋疼。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://technology.chtsai.org/mmseg/&#34;&gt;算法的原文&lt;/a&gt;，中文有翻译的。其实就是用了四个判断的法则，需要的可以查文档，这里略过&lt;/p&gt;

&lt;p&gt;算法的说明也是满大街都是,简单给个&lt;a href=&#34;http://www.byywee.com/page/M0/S602/602088.html&#34;&gt;博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;斯坦福大学有个具体的&lt;a href=&#34;http://nlp.stanford.edu/software/segmenter.shtml&#34;&gt;实现包&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里举个正向最大匹配和mmseg算法对同样的文本切割后的效果吧：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;南京市长江大桥欢迎您

&lt;ul&gt;
&lt;li&gt;正向最大匹配：南京/市长/江/大桥/欢迎您&lt;/li&gt;
&lt;li&gt;mmseg的方法：南京市/长江大桥/欢迎您&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;研究生命科学

&lt;ul&gt;
&lt;li&gt;正向最大匹配：研究生/命/科学&lt;/li&gt;
&lt;li&gt;mmseg的方法：研究/生命/科学&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;长春市长春药店

&lt;ul&gt;
&lt;li&gt;正向最大匹配：长春/市长/春药/店&lt;/li&gt;
&lt;li&gt;mmseg的方法：长春市/长春/药店&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;台湾国中学生

&lt;ul&gt;
&lt;li&gt;正向最大匹配：台湾国/中学生&lt;/li&gt;
&lt;li&gt;mmseg的方法：台湾/国中/学生&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当然这些匹配的结果都取决于您的字典&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;mmseg方法:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;mmseg方法&lt;/h1&gt;

&lt;h2 id=&#34;匹配方法:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;&lt;strong&gt;匹配方法&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&#34;1-simple方法:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;1. Simple方法&lt;/h3&gt;

&lt;p&gt;即简单的正向匹配，根据开头的字，列出所有可能的结果。&lt;/p&gt;

&lt;p&gt;比如“一个劲儿的说话”，可以得到&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;一个&lt;br /&gt;
一个劲&lt;br /&gt;
一个劲儿&lt;br /&gt;
一个劲儿的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这四个匹配结果（假设这四个词都包含在词典里）。&lt;/p&gt;

&lt;h3 id=&#34;2-complex方法:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;2. Complex方法&lt;/h3&gt;

&lt;p&gt;匹配出所有的“三个词的词组”（原文中使用了chunk，这里感觉用“词组”比较合适），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合。&lt;/p&gt;

&lt;p&gt;比如“研究生命起源”，可以得到&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;研 \究 \生&lt;br /&gt;
研 \究 \生命&lt;br /&gt;
研究生 \命 \起源&lt;br /&gt;
研究 \生命 \起源&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这些“词组”（根据词典，可能远不止这些，仅此举例）&lt;/p&gt;

&lt;h2 id=&#34;消除歧义的规则:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;&lt;strong&gt;消除歧义的规则&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&#34;1-maximum-matching-最大匹配:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;1. Maximum matching (最大匹配)&lt;/h3&gt;

&lt;p&gt;有两种情况，分别对应于使用“simple”和“complex”的匹配方法。&lt;/p&gt;

&lt;p&gt;对“simple”匹配方法，选择长度最大的词，用在上文的例子中即选择“一个劲儿的”&lt;/p&gt;

&lt;p&gt;对“complex”匹配方法，选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，上文的例子中即“研究生 命 起源”中的“研究生”，或者“研究 生命 起源”中的“研究”。&lt;/p&gt;

&lt;h3 id=&#34;2-largest-average-word-length-最大平均词语长度:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;2. Largest average word length（最大平均词语长度）&lt;/h3&gt;

&lt;p&gt;经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数/词语数量）。&lt;/p&gt;

&lt;p&gt;比如“生活水平”，可能得到如下词组：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;生 \活水 \平 (&lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;=1.33)&lt;br /&gt;
生活 \水 \平 (&lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;=1.33)&lt;br /&gt;
生活 \水平 (&lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;=2)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据此规则，就可以确定选择“生活水平”这个词组&lt;/p&gt;

&lt;h3 id=&#34;3-smallest-variance-of-word-lengths-词语长度的最小变化率:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;3. Smallest variance of word lengths（词语长度的最小变化率）&lt;/h3&gt;

&lt;p&gt;由于词语长度的变化率可以由&lt;a href=&#34;http://baike.baidu.com/view/78339.htm&#34;&gt;标准差&lt;/a&gt;反映，所以此处直接套用标准差公式即可。&lt;/p&gt;

&lt;p&gt;比如:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;研究 \生命 \起源 （标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0）&lt;br /&gt;
研究生 \命 \起源 （标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;于是选择“研究 \生命 \起源”这个词组。&lt;/p&gt;

&lt;h3 id=&#34;4-largest-sum-of-degree-of-morphemic-freedom-of-one-character-words:c66c89eb3c614731ea63bf4d9bea6b95&#34;&gt;4. Largest sum of degree of morphemic freedom of one-character words&lt;/h3&gt;

&lt;p&gt;其中degree of morphemic freedom可以用一个数学公式表达：log(frequency)，即词频的自然对数（这里log表示数学中的ln）。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。&lt;/p&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;设施 \和服 \务&lt;br /&gt;
设施 \和 \服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施 /和 /服务”。&lt;br /&gt;
也许会问为什么要对“词频”取自然对数呢？可以这样理解，词组中单字词词频总和可能一样，但是实际的效果并不同.&lt;/p&gt;

&lt;p&gt;比如:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A \BBB \C （单字词词频，A:3， C:7）&lt;br /&gt;
DD \E \F （单字词词频，E:5，F:5）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），但实际上不同的词频范围所表示的效果也不同，所以这里取自然对数，以表区分（ln(3)+ln(7) &amp;lt; ln(5)+ln(5)， 3.0445&amp;lt;3.2189）。&lt;/p&gt;

&lt;p&gt;这个四个过滤规则中，如果使用simple的匹配方法，只能使用第一个规则过滤，如果使用complex的匹配方法，则四个规则都可以使用。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。（simple的匹配方法实质上就是正向最大匹配，实际中很少只用这一个方法）。&lt;/p&gt;

&lt;p&gt;看到这里也许对MMSEG的分词方法有了一个大致的了解，正如文章开头所述，它是一个“直观”的分词方法。它把一个句子“尽可能长（这里的长，是指所切分的词尽可能的长）”“尽可能均匀”的区切分，稍微想象一下，便感觉与中文的语法习惯比较相符。如果对分词精度要求不是特别高，MMSEG是一个简单、可行、快速的方法。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;大家在看了上面的博客，理解了什么是mmseg算法再往下看&lt;br /&gt;
具体Python代码如下：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cedarbye.github.io/code/20160324/mmseg.py&#34;&gt;mmseg.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;此代码参照google code的一个代码，经过了我自己的修改和自己去其的理解&lt;br /&gt;
代码结构说明：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Word类就相当于C语言中的结构体，用来存单词和词频的&lt;br /&gt;
Chunk类是用来实现具体的切割判断方法的前期预处理计算的&lt;br /&gt;
ComplexCompare类是用来具体实现mmseg算法的四种评估方法的&lt;br /&gt;
有几个全局变量和全局函数是用来加载字典的，用全局是为了不让字典多次的加载&lt;br /&gt;
Analysis类是用来具体实现切割算法的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是测试代码&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cedarbye.github.io/code/20160324/mmseg_test.py&#34;&gt;mmseg_test.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;转载自&lt;a href=&#34;http://www.aiuxian.com/article/p-1976364.html&#34;&gt;爱悠闲&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习——BIRCH算法</title>
      <link>https://cedarbye.github.io/post/2015/2015_04_15BIRCH%E7%AE%97%E6%B3%95/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2015/2015_04_15BIRCH%E7%AE%97%E6%B3%95/</guid>
      <description>

&lt;h2 id=&#34;1-什么是birch算法:f94b0b3b1309c93b1ba6037086c08efc&#34;&gt;1. 什么是BIRCH算法？&lt;/h2&gt;

&lt;p&gt;BIRCH（Balanced Iterative Reducing and Clustering using Hierarchies）天生就是为处理超大规模（至少要让你的内存容不下）的数据集而设计的，它可以在任何给定的内存下运行。&lt;/p&gt;

&lt;p&gt;BIRCH算法的过程就是要把待分类的数据插入一棵树中，并且原始数据都在叶子节点上。&lt;/p&gt;

&lt;p&gt;在这棵树中有3种类型的节点：Nonleaf、Leaf、MinCluster，Root可能是一种Nonleaf，也可能是一种Leaf。所有的Leaf放入一个双向链表中。每一个节点都包含一个CF值，CF是一个三元组，其中data point instance的个数，LS和SS是与数据点同维度的向量，LS是线性和，SS是平方和。比如有一个MinCluster里包含3个数据点(1,2,3)，(4,5,6)，(7,8,9)，则&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;N=3&lt;/p&gt;

&lt;p&gt;LS=(1+4+7,2+5+8,3+6+9)=(12,15,18)&lt;/p&gt;

&lt;p&gt;SS=(1+16+49,4+25+64,9+36+81)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所谓两簇合并只需要两个对应的CF相加那可&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CF1 + CF2 = (N1 + N2 , LS1 + LS2, SS1 + SS2)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个节点的CF值就是其所有孩子节点CF值之和，以每个节点为根节点的子树都可以看成 是一个簇。&lt;/p&gt;

&lt;p&gt;Nonleaf、Leaf、MinCluster都是有大小限制的，Nonleaf的孩子节点不能超过B个，Leaf最多只能有L个MinCluster，而一个MinCluster的直径不能超过T。&lt;/p&gt;

&lt;h2 id=&#34;2-算法实现:f94b0b3b1309c93b1ba6037086c08efc&#34;&gt;2. 算法实现&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;算法起初，我们扫描数据库，拿到第一个data point instance&amp;ndash;（1,2,3）,我们创建一个空的Leaf和MinCluster，把点（1,2,3）的id值放入Mincluster，更新MinCluster的CF值为（1,（1,2,3），（1,4,9）），把MinCluster作为Leaf的一个孩子，更新Leaf的CF值为（1,（1,2,3），（1,4,9））。实际上只要往树中放入一个CF（这里我们用CF作为Nonleaf、Leaf、MinCluster的统称），就要更新从Root到该叶子节点的路径上所有节点的CF值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当又有一个数据点要插入树中时，把这个点封装为一个MinCluster（这样它就有了一个CF值），把新到的数据点记为CF_new，我们拿到树的根节点的各个孩子节点的CF值，根据D2来找到CF_new与哪个节点最近，就把CF_new加入那个子树上面去。这是一个递归的过程。递归的终止点是要把CF_new加入到一个MinCluster中，如果加入之后MinCluster的直径没有超过T，则直接加入，否则譔CF_new要单独作为一个簇，成为MinCluster的兄弟结点。插入之后注意更新该节点及其所有祖先节点的CF值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;插入新节点后，可能有些节点的孩子数大于了B（或L），此时该节点要分裂。对于Leaf，它现在有L+1个MinCluster，我们要新创建一个Leaf，使它作为原Leaf的兄弟结点，同时注意每新创建一个Leaf都要把它插入到双向链表中。L+1个MinCluster要分到这两个Leaf中，怎么分呢？找出这L+1个MinCluster中距离最远的两个Cluster（根据D2），剩下的Cluster看离哪个近就跟谁站在一起。分好后更新两个Leaf的CF值，其祖先节点的CF值没有变化，不需要更新。这可能导致祖先节点的递归分裂，因为Leaf分裂后恰好其父节点的孩子数超过了B。Nonleaf的分裂方法与Leaf的相似，只不过产生新的Nonleaf后不需要把它放入一个双向链表中。如果是树的根节点要分裂，则树的高度加1。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;h2 id=&#34;3-总结:f94b0b3b1309c93b1ba6037086c08efc&#34;&gt;3. 总结&lt;/h2&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;省内存。叶子节点放在磁盘分区上，非叶子节点仅仅是存储了一个CF值，外加指向父节点和孩子节点的指针。&lt;/li&gt;
&lt;li&gt;快。合并两个两簇只需要两个CF算术相加即可；计算两个簇的距离只需要用到(N,LS,SS)这三个值足矣。&lt;/li&gt;
&lt;li&gt;一遍扫描数据库即可建立B树。&lt;/li&gt;
&lt;li&gt;可识别噪声点。建立好B树后把那些包含数据点少的MinCluster当作outlier。&lt;/li&gt;
&lt;li&gt;由于B树是高度平衡的，所以在树上进行插入或查找操作很快。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;结果依赖于数据点的插入顺序。本属于同一个簇的点可能由于插入顺序相差很远而分到不同的簇中，即使同一个点在不同的时刻被插入，也会被分到不同的簇中。&lt;/li&gt;
&lt;li&gt;对非球状的簇聚类效果不好。这取决于簇直径和簇间距离的计算方法。&lt;/li&gt;
&lt;li&gt;对高维数据聚类效果不好。&lt;/li&gt;
&lt;li&gt;由于每个节点只能包含一定数目的子节点，最后得出来的簇可能和自然簇相差很大。&lt;/li&gt;
&lt;li&gt;BIRCH适合于处理需要数十上百小时聚类的数据，但在整个过程中算法一旦中断，一切必须从头再来。&lt;/li&gt;
&lt;li&gt;局部性也导致了BIRCH的聚类效果欠佳。当一个新点要插入B树时，它只跟很少一部分簇进行了相似性（通过计算簇间距离）比较，高的efficient导致低的effective。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>机器学习———OPTICS算法</title>
      <link>https://cedarbye.github.io/post/2015/2015_04_12OPTICS%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 12 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2015/2015_04_12OPTICS%E7%AE%97%E6%B3%95/</guid>
      <description>

&lt;h2 id=&#34;1-什么是optics算法:2f3b26bd381ea7b4d97c66208712e19d&#34;&gt;1. 什么是OPTICS算法&lt;/h2&gt;

&lt;p&gt;在前面介绍的DBSCAN算法中，有两个初始参数E（邻域半径）和minPts(E邻域最小点数)需要用户手动设置输入，并且聚类的类簇结果对这两个参数的取值非常敏感，不同的取值将产生不同的聚类结果，其实这也是大多数其他需要初始化参数聚类算法的弊端。&lt;br /&gt;
为了克服DBSCAN算法这一缺点，提出了OPTICS算法（Ordering Points to identify the clustering structure）。OPTICS并 不显示的产生结果类簇，而是为聚类分析生成一个增广的簇排序（比如，以可达距离为纵轴，样本点输出次序为横轴的坐标图），这个排序代表了各样本点基于密度 的聚类结构。它包含的信息等价于从一个广泛的参数设置所获得的基于密度的聚类，换句话说，从这个排序中可以得到基于任何参数E和minPts的DBSCAN算法的聚类结果。&lt;/p&gt;

&lt;p&gt;** OPTICS两个概念 **&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;核心距离：
对象p的核心距离是指是p成为核心对象的最小E’。如果p不是核心对象，那么p的核心距离没有任何意义。&lt;/li&gt;
&lt;li&gt;可达距离：
对象q到对象p的可达距离是指p的核心距离和p与q之间欧几里得距离之间的较大值。如果p不是核心对象，p和q之间的可达距离没有意义。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;例如：
假设邻域半径E=2, minPts=3，存在点A(2,3),B(2,4),C(1,4),D(1,3),E(2,2),F(3,2)&lt;/p&gt;

&lt;p&gt;点A为核心对象，在A的E领域中有点{A,B,C,D,E,F}，其中A的核心距离为E’=1，因为在点A的E’邻域中有点{A,B,D,E}&amp;gt;3;&lt;/p&gt;

&lt;p&gt;点F到核心对象点A的可达距离为2^(&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;)，因为A到F的欧几里得距离2^(&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;)，大于点A的核心距离1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-算法描述:2f3b26bd381ea7b4d97c66208712e19d&#34;&gt;3. 算法描述&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;算法：OPTICS&lt;/li&gt;
&lt;li&gt;输入：样本集D, 邻域半径E, 给定点在E领域内成为核心对象的最小领域点数MinPts&lt;/li&gt;
&lt;li&gt;输出：具有可达距离信息的样本点输出排序&lt;/li&gt;
&lt;li&gt;方法：&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建两个队列，有序队列和结果队列。（有序队列用来存储核心对象及其该核心对象的直接可达对象，并按可达距离升序排列；结果队列用来存储样本点的输出次序）；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果所有样本集D中所有点都处理完毕，则算法结束。否则，选择一个未处理（即不在结果队列中）且为核心对象的样本点，找到其所有直接密度可达样本点，如过该样本点不存在于结果队列中，则将其放入有序队列中，并按可达距离排序；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果有序队列为空，则跳至步骤2，否则，从有序队列中取出第一个样本点（即可达距离最小的样本点）进行拓展，并将取出的样本点保存至结果队列中，如果它不存在结果队列当中的话。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;3.1 判断该拓展点是否是核心对象，如果不是，回到步骤3，否则找到该拓展点所有的直接密度可达点；&lt;/p&gt;

&lt;p&gt;3.2 判断该直接密度可达样本点是否已经存在结果队列，是则不处理，否则下一步；&lt;/p&gt;

&lt;p&gt;3.3 如果有序队列中已经存在该直接密度可达点，如果此时新的可达距离小于旧的可达距离，则用新可达距离取代旧可达距离，有序队列重新排序；&lt;/p&gt;

&lt;p&gt;3.4 如果有序队列中不存在该直接密度可达样本点，则插入该点，并对有序队列重新排序；&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;4.算法结束，输出结果队列中的有序样本点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里的E和MinPts只是起到算法辅助作用，也就是说E和MinPts的细微变化并不会影响到样本点的相对输出顺序，这对我们分析聚类结果是没有任何影响。&lt;/p&gt;

&lt;h2 id=&#34;3-总结:2f3b26bd381ea7b4d97c66208712e19d&#34;&gt;3. 总结&lt;/h2&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无须事先知道簇数&lt;/li&gt;
&lt;li&gt;不续约标准的方法或非常鲁棒的参数&lt;/li&gt;
&lt;li&gt;计算簇的完整的层次结构&lt;/li&gt;
&lt;li&gt;方法包含很好的结果可视化&lt;/li&gt;
&lt;li&gt;随后可以得到“平坦的”划分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可能不具有良好的伸缩性&lt;/li&gt;
&lt;li&gt;复杂度高（n*nlong(n*n)）&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>机器学习——K-means算法</title>
      <link>https://cedarbye.github.io/post/2015/2015_04_05K-means%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 05 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2015/2015_04_05K-means%E7%AE%97%E6%B3%95/</guid>
      <description>

&lt;h2 id=&#34;1-什么是k-means算法:7f423d20730d9400479993134038f0af&#34;&gt;1. 什么是K-means算法&lt;/h2&gt;

&lt;p&gt;基本思想是初始随机给定K个簇中心，按照最邻近原则把待分类样本点分到各个簇。然后按平均法重新计算各个簇的质心，从而确定新的簇心。一直迭代，直到簇心的移动距离小于某个给定的值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;选择K个点作为初始质心&lt;br /&gt;
repeat&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;将每个点指派到最近的质心，形成K个簇&lt;br /&gt;
重新计算每个簇的质心&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;until 簇不发生变化或达到最大迭代次数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;时间复杂度&lt;/em&gt; ：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数&lt;/p&gt;

&lt;p&gt;&lt;em&gt;空间复杂度&lt;/em&gt; ：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数&lt;/p&gt;

&lt;h2 id=&#34;2-算法过程:7f423d20730d9400479993134038f0af&#34;&gt;2. 算法过程&lt;/h2&gt;

&lt;p&gt;k均值算法的计算过程非常直观：&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;从D中随机取k个元素，作为k个簇的各自的中心。&lt;/li&gt;
&lt;li&gt;分别计算剩下的元素到k个簇中心的相异度，将这些元素分别划归到相异度最低的簇。&lt;/li&gt;
&lt;li&gt;根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均数。&lt;/li&gt;
&lt;li&gt;将D中全部元素按照新的中心重新聚类。&lt;/li&gt;
&lt;li&gt;重复第4步，直到聚类结果不再变化。&lt;/li&gt;
&lt;li&gt;将结果输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;h2 id=&#34;3-总结:7f423d20730d9400479993134038f0af&#34;&gt;3. 总结&lt;/h2&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;原理简单，实现容易&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;收敛太慢&lt;/li&gt;
&lt;li&gt;算法复杂度高O(nkt)&lt;/li&gt;
&lt;li&gt;不能发现非凸形状的簇，或大小差别很大的簇&lt;/li&gt;
&lt;li&gt;样本存在均值（限定数据种类）&lt;/li&gt;
&lt;li&gt;需先确定k的个数&lt;/li&gt;
&lt;li&gt;对噪声和离群点敏感&lt;/li&gt;
&lt;li&gt;最重要是结果不一定是全局最优，只能保证局部最优。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kmenas算法试图找到使平凡误差准则函数最小的簇。当潜在的簇形状是凸面的，簇与簇之间区别较明显，且簇大小相近时，其聚类结果较理想。前面提到，该算法时间复杂度为O(tKmn)，与样本数量线性相关，所以，对于处理大数据集合，该算法非常高效，且伸缩性较好。但该算法除了要事先确定簇数K和对初始聚类中心敏感外，经常以局部最优结束，同时对“噪声”和孤立点敏感，并且该方法不适于发现非凸面形状的簇或大小差别很大的簇。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习——支持向量机(SVM)算法</title>
      <link>https://cedarbye.github.io/post/2015/2015_03_21%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%28SVM%29%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sat, 21 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2015/2015_03_21%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%28SVM%29%E7%AE%97%E6%B3%95/</guid>
      <description>

&lt;p&gt;关于SVM的论文、书籍都非常的多，引用强哥的话“SVM是让应用数学家真正得到应用的一种算法”。SVM对于大部分的普通人来说，要完全理解其中的数学是非常困难的，所以要让这些普通人理解，得要把里面的数学知识用简单的语言去讲解才行。而且想明白了这些数学，对学习其他的内容也是大有裨益的。我就是属于绝大多数的普通人，为了看明白SVM，看了不少的资料，这里把我的心得分享分享。&lt;/p&gt;

&lt;p&gt;其实现在能够找到的，关于SVM的中文资料已经不少了，不过个人觉得，每个人的理解都不太一样，所以还是决定写一写，一些雷同的地方肯定是不可避免的，不过还是希望能够写出一点与别人不一样的地方吧。另外本文准备不谈太多的数学（因为很多文章都谈过了），尽量简单地给出结论，就像题目一样-机器学习中的算法（之前叫做机器学习中的数学），所以本系列的内容将更偏重应用一些。如果想看更详细的数学解释，可以看看参考文献中的资料。&lt;/p&gt;

&lt;h2 id=&#34;1-线性分类器:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;1. 线性分类器：&lt;/h2&gt;

&lt;p&gt;首先给出一个非常非常简单的分类问题（线性可分），我们要用一条直线，将下图中黑色的点和白色的点分开，很显然，图上的这条直线就是我们要求的直线之一（可以有无数条这样的直线）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032101.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;假如说，我们令黑色的点 = -1， 白色的点 = +1，直线f(x) = w.x + b这儿的x、w是向量，其实写成这种形式也是等价的f(x) = w1x1 + w2x2 … + wnxn + b， 当向量x的维度=2的时候，f(x)表示二维空间中的一条直线，当x的维度=3的时候，f(x)表示3维空间中的一个平面，当x的维度=n &amp;gt; 3的时候，表示n维空间中的n-1维超平面。这些都是比较基础的内容，如果不太清楚，可能需要复习一下微积分、线性代数的内容。&lt;/p&gt;

&lt;p&gt;刚刚说了，我们令黑色白色两类的点分别为+1, -1，所以当有一个新的点x需要预测属于哪个分类的时候，我们用sgn(f(x))，就可以预测了，sgn表示符号函数，当f(x) &amp;gt; 0的时候，sgn(f(x)) = +1, 当f(x) &amp;lt; 0的时候sgn(f(x)) = –1。&lt;/p&gt;

&lt;p&gt;但是，我们怎样才能取得一个最优的划分直线f(x)呢？下图的直线表示几条可能的f(x)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032102.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个很直观的感受是，让这条直线到给定样本中最近的点最远，这句话读起来比较拗口，下面给出几个图，来说明一下：&lt;/p&gt;

&lt;p&gt;第一种分法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032103.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二种分法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032104.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这两种分法哪种更好呢？从直观上来说，就是分割的间隙越大越好，把两个类别的点分得越开越好。就像我们平时判断一个人是男还是女，就是很难出现分错的情况，这就是男、女两个类别之间的间隙非常的大导致的，让我们可以更准确的进行分类。在SVM中，称为&lt;strong&gt;Maximum Marginal&lt;/strong&gt;，是SVM的一个理论基础之一。选择使得间隙最大的函数作为分割平面是由很多道理的，比如说从概率的角度上来说，就是使得置信度最小的点置信度最大（听起来很拗口），从实践的角度来说，这样的效果非常好，等等。这里就不展开讲，作为一个结论就ok了，:)&lt;/p&gt;

&lt;p&gt;上图被红色和蓝色的线圈出来的点就是所谓的支持向量(support vector)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032105.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图就是一个对之前说的类别中的间隙的一个描述。Classifier Boundary就是f(x)，红色和蓝色的线（plus plane与minus plane）就是support vector所在的面，红色、蓝色线之间的间隙就是我们要最大化的分类间的间隙。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032106.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里直接给出M的式子：（从高中的解析几何就可以很容易的得到了，也可以参考后面Moore的ppt）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032107.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;另外支持向量位于 wx + b = 1 与wx + b = -1的直线上，我们在前面乘上一个该点所属的类别y（还记得吗? y不是+1就是-1），就可以得到支持向量的表达式为：y(wx + b) = 1，这样就可以更简单的将支持向量表示出来了。&lt;/p&gt;

&lt;p&gt;当支持向量确定下来的时候，分割函数就确定下来了，两个问题是等价的。得到支持向量，还有一个作用是，让支持向量后方那些点就不用参与计算了。这点在后面将会更详细的讲讲。&lt;/p&gt;

&lt;p&gt;在这个小节的最后，给出我们要优化求解的表达式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032108.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;||w||的意思是w的二范数，跟上面的M表达式的分母是一个意思，之前得到，M = 2 / ||w||，最大化这个式子等价于最小化||w||, 另外由于||w||是一个单调函数，我们可以对其加入平方，和前面的系数，熟悉的同学应该很容易就看出来了，这个式子是为了方便求导。&lt;/p&gt;

&lt;p&gt;这个式子有还有一些限制条件，完整的写下来，应该是这样的：（&lt;em&gt;原问题&lt;/em&gt; ）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032109.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;s.t的意思是subject to，也就是在后面这个限制条件下的意思，这个词在svm的论文里面非常容易见到。这个其实是一个带约束的二次规划(quadratic programming, QP)问题，是一个凸问题，凸问题就是指的不会有局部最优解，可以想象一个漏斗，不管我们开始的时候将一个小球放在漏斗的什么位置，这个小球最终一定可以掉出漏斗，也就是得到全局最优解。s.t.后面的限制条件可以看做是一个凸多面体，我们要做的就是在这个凸多面体中找到最优解。这些问题这里不展开，因为展开的话，一本书也写不完。如果有疑问请看看wikipedia。&lt;/p&gt;

&lt;h2 id=&#34;2-转化为对偶问题-并优化求解:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;2. 转化为对偶问题，并优化求解:&lt;/h2&gt;

&lt;p&gt;这个优化问题可以用&lt;em&gt;拉格朗日乘子法&lt;/em&gt; 去解，使用了&lt;em&gt;KKT条件&lt;/em&gt; 的理论，这里直接作出这个式子的拉格朗日目标函数：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032110.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;求解这个式子的过程需要&lt;em&gt;拉格朗日对偶性&lt;/em&gt; 的相关知识（另外pluskid也有一篇文章专门讲这个问题），并且有一定的公式推导，如果不感兴趣，可以直接跳到后面用&lt;em&gt;蓝色公式&lt;/em&gt; 表示的结论，该部分推导主要参考自plukids的文章。&lt;/p&gt;

&lt;p&gt;首先让L关于w，b最小化，分别令L关于w，b的偏导数为0，得到关于原问题的一个表达式&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032111.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;将两式带回L(w,b,a)得到对偶问题的表达式&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032112.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;新问题加上其限制条件是（&lt;em&gt;对偶问题&lt;/em&gt; ）:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032113.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个就是我们需要最终优化的式子。至此，得到了线性可分问题的优化式子。&lt;/p&gt;

&lt;p&gt;求解这个式子，有很多的方法，比如&lt;em&gt;SMO&lt;/em&gt; 等等，个人认为，求解这样的一个带约束的凸优化问题与得到这个凸优化问题是比较独立的两件事情，所以在这篇文章中准备完全不涉及如何求解这个话题，如果之后有时间可以补上一篇文章来谈谈:)。&lt;/p&gt;

&lt;h2 id=&#34;3-线性不可分的情况-软间隔:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;3. 线性不可分的情况（软间隔）：&lt;/h2&gt;

&lt;p&gt;接下来谈谈线性不可分的情况，因为线性可分这种假设实在是太有局限性了：&lt;/p&gt;

&lt;p&gt;下图就是一个典型的线性不可分的分类图，我们没有办法用一条直线去将其分成两个区域，每个区域只包含一种颜色的点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032114.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;要想在这种情况下的分类器，有两种方式，一种是用曲线去将其完全分开，曲线就是一种非线性的情况，跟之后将谈到的核函数有一定的关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032115.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;另外一种还是用直线，不过不用去保证可分性，就是包容那些分错的情况，不过我们得加入惩罚函数，使得点分错的情况越合理越好。其实在很多时候，不是在训练的时候分类函数越完美越好，因为训练函数中有些数据本来就是噪声，可能就是在人工加上分类标签的时候加错了，如果我们在训练（学习）的时候把这些错误的点学习到了，那么模型在下次碰到这些错误情况的时候就难免出错了（假如老师给你讲课的时候，某个知识点讲错了，你还信以为真了，那么在考试的时候就难免出错）。这种学习的时候学到了“噪声”的过程就是一个过拟合（over-fitting），这在机器学习中是一个大忌，我们宁愿少学一些内容，也坚决杜绝多学一些错误的知识。还是回到主题，用直线怎么去分割线性不可分的点：&lt;/p&gt;

&lt;p&gt;我们可以为分错的点加上一点惩罚，对一个分错的点的惩罚函数就是这个点到其正确位置的距离：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032116.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图中，蓝色、红色的直线分别为支持向量所在的边界，绿色的线为决策函数，那些紫色的线表示分错的点到其相应的决策面的距离，这样我们可以在原函数上面加上一个惩罚函数，并且带上其限制条件为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032117.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;公式中蓝色的部分为在线性可分问题的基础上加上的惩罚函数部分，当xi在正确一边的时候，ε=0，R为全部的点的数目，C是一个由用户去指定的系数，表示对分错的点加入多少的惩罚，当C很大的时候，分错的点就会更少，但是过拟合的情况可能会比较严重，当C很小的时候，分错的点可能会很多，不过可能由此得到的模型也会不太正确，所以如何选择C是有很多学问的，不过在大部分情况下就是通过经验尝试得到的。&lt;/p&gt;

&lt;p&gt;接下来就是同样的，求解一个拉格朗日对偶问题，得到一个原问题的对偶问题的表达式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032118.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;蓝色的部分是与线性可分的对偶问题表达式的不同之处。在线性不可分情况下得到的对偶问题，不同的地方就是α的范围从[0, +∞)，变为了[0, C]，增加的惩罚ε没有为对偶问题增加什么复杂度。&lt;/p&gt;

&lt;h2 id=&#34;4-核函数:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;4. 核函数：&lt;/h2&gt;

&lt;p&gt;刚刚在谈不可分的情况下，提了一句，如果使用某些非线性的方法，可以得到将两个分类完美划分的曲线，比如接下来将要说的核函数。&lt;/p&gt;

&lt;p&gt;我们可以让空间从原本的线性空间变成一个更高维的空间，在这个高维的线性空间下，再用一个超平面进行划分。这儿举个例子，来理解一下如何利用空间的维度变得更高来帮助我们分类的（例子以及图片来自pluskid的kernel函数部分）：&lt;/p&gt;

&lt;p&gt;下图是一个典型的线性不可分的情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032119.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是当我们把这两个类似于椭圆形的点映射到一个高维空间后，映射函数为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032120.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;用这个函数可以将上图的平面中的点映射到一个三维空间（z1,z2,z3)，并且对映射后的坐标加以旋转之后就可以得到一个线性可分的点集了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032121.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;用另外一个哲学例子来说：世界上本来没有两个完全一样的物体，对于所有的两个物体，我们可以通过增加维度来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入 页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。当维度增加到无限维的时候，一定可以让任意的两个物体可分了。&lt;/p&gt;

&lt;p&gt;回忆刚刚得到的对偶问题表达式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032122.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以将红色这个部分进行改造，令：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032123.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个式子所做的事情就是将线性的空间映射到高维的空间,k(x, xj)有很多种，下面是比较典型的两种：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cedarbye.github.io/img/2016032124.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面这个核称为多项式核，下面这个核称为高斯核，高斯核甚至是将原始空间映射为无穷维空间，另外核函数有一些比较好的性质，比如说不会比线性条件下增加多少额外的计算量，等等，这里也不再深入。一般对于一个问题，不同的核函数可能会带来不同的结果，一般是需要尝试来得到的。&lt;/p&gt;

&lt;h2 id=&#34;5-一些其他的问题:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;5. 一些其他的问题：&lt;/h2&gt;

&lt;h3 id=&#34;如何进行多分类问题:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;如何进行多分类问题：&lt;/h3&gt;

&lt;p&gt;上面所谈到的分类都是2分类的情况，当N分类的情况下，主要有两种方式：一种是1vs(N–1)一种是1vs1。&lt;/p&gt;

&lt;p&gt;前一种方法我们需要训练N个分类器，第i个分类器是看看是属于分类i还是属于分类i的补集（出去i的N-1个分类）。&lt;/p&gt;

&lt;p&gt;后一种方式我们需要训练N*(N–1)/2个分类器，分类器(i,j)能够判断某个点是属于i还是属于j。&lt;/p&gt;

&lt;p&gt;这种处理方式不仅在SVM中会用到，在很多其他的分类中也是被广泛用到，从林教授（libsvm的作者）的结论来看，1vs1的方式要优于1vs(N–1)。&lt;/p&gt;

&lt;h3 id=&#34;svm会overfitting吗:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;SVM会overfitting吗？&lt;/h3&gt;

&lt;p&gt;SVM避免overfitting，一种是调整之前说的惩罚函数中的C，另一种其实从式子上来看，min ||w||^2这个看起来是不是很眼熟？在最小二乘法回归的时候，我们看到过这个式子，这个式子可以让函数更平滑，所以SVM是一种不太容易over-fitting的方法。&lt;/p&gt;

&lt;h2 id=&#34;6-总结:8ec9919cf2ac15a793c4a896048e099d&#34;&gt;6. 总结&lt;/h2&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以解决小样本情况下的机器学习问题。&lt;/li&gt;
&lt;li&gt;可以提高泛化性能。&lt;/li&gt;
&lt;li&gt;可以解决高维问题。&lt;/li&gt;
&lt;li&gt;可以解决非线性问题。&lt;/li&gt;
&lt;li&gt;可以避免神经网络结构选择和局部极小点问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对缺失数据敏感。&lt;/li&gt;
&lt;li&gt;对非线性问题没有通用解决方案，必须谨慎选择Kernelfunction来处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;参考文档：
主要的参考文档来自4个地方，wikipedia（在文章中已经给出了超链接了），pluskid关于SVM的博文，Andrew moore的ppt（文章中不少图片都是引用或者改自Andrew Moore的ppt，以及prml&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html&#34;&gt;来源leftnoteasy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习导论</title>
      <link>https://cedarbye.github.io/post/2015/2015_01_24%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</link>
      <pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://cedarbye.github.io/post/2015/2015_01_24%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</guid>
      <description>

&lt;p&gt;机器学习（Machine Learning， ML）是什么，作为一个MLer，经常难以向大家解释何为ML。久而久之，发现要理解或解释机器学习是什么，可以从机器学习可以解决的问题这个角度来说。对于MLers，理解ML解决的问题的类型也有助于我们更好的准备数据和选择算法。&lt;/p&gt;

&lt;h3 id=&#34;十个机器学习问题样例:a026595abc6cafd88e910b141621a264&#34;&gt;十个机器学习问题样例&lt;/h3&gt;

&lt;p&gt;想入门机器学习的同学，经常会去看一些入门书，比如《集体智慧编程》、《机器学习实战》、《数据挖掘》、《推荐系统实践》等。看书的过程中，经常性的会看到如下样例：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;垃圾邮件识别&lt;/li&gt;
&lt;li&gt;信用卡交易异常检测&lt;/li&gt;
&lt;li&gt;手写数字识别&lt;/li&gt;
&lt;li&gt;语音识别&lt;/li&gt;
&lt;li&gt;人脸检测&lt;/li&gt;
&lt;li&gt;商品推荐&lt;/li&gt;
&lt;li&gt;疾病检测（根据以往病例记录，确定病人是否患病）&lt;/li&gt;
&lt;li&gt;股票预测&lt;/li&gt;
&lt;li&gt;用户分类（根据用户行为判断该用户是否会转化为付费用户）&lt;/li&gt;
&lt;li&gt;形状检测（根据用户在手写板上上画得形状，确定用户画的到底是什么形状）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此，当再有人问ML是什么的时候，就可以说这个是ML可以handle的，这个问题ML也可以handle，blahblah。&lt;/p&gt;

&lt;h3 id=&#34;机器学习问题类型:a026595abc6cafd88e910b141621a264&#34;&gt;机器学习问题类型&lt;/h3&gt;

&lt;p&gt;对问题进行分类，好处就在于可以更好的把握问题的本质，更好的知道什么类型的算法需要用到。&lt;/p&gt;

&lt;p&gt;一般有四大类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分类（classification）：有一些已经标注好类别的数据，在标注好的数据上建模，对于新样本，判断它的类别。如垃圾邮件识别&lt;/li&gt;
&lt;li&gt;回归（regression）：有一些已经标注好的数据，标注值与分类问题不同，分类问题的标注是离散值，而回归问题中的标注是实数，在标注好的数据上建模，对于新样本，得到它的标注值。如股票预测。&lt;/li&gt;
&lt;li&gt;聚类（clustering）：数据没有被标注，但是给出了一些相似度衡量标准，可以根据这些标准将数据进行划分。如在一堆未给出名字的照片中，自动的将同一个人的照片聚集到一块。&lt;/li&gt;
&lt;li&gt;规则抽取（rule extraction）：发现数据中属性之间的统计关系，而不只是预测一些事情。如啤酒和尿布。
机器学习算法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;知道了机器学习要解决的问题后，就可以思考针对某一个问题，需要采集的数据的类型和可以使用的机器学习算法，机器学习发展到今天，诞生了很多算法，在实际应用中往往问题在于算法的选择，在本文中，使用两种标准对算法进行分类，即学习方式和算法之间的相似性。&lt;/p&gt;

&lt;h3 id=&#34;学习方式-learning-style:a026595abc6cafd88e910b141621a264&#34;&gt;学习方式（Learning Style）&lt;/h3&gt;

&lt;p&gt;在ML中，只有几个主流的学习方式，在下面的介绍中，使用一些算法和问题的样例来对这些方式进行解释说明。按照学习方式对机器学习算法进行分类可以使我们更多的思考输入数据在算法中的角色和使用模型前需要的准备工作，对我们选择最适合的模型有很好的指导作用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;监督学习（supervised learning）：输入数据都有一个类别标记或结果标记，被称作训练数据，比如垃圾邮件与非垃圾邮件、某时间点的股票价格。模型由训练过程得到，利用模型，可以对新样本做出推测，并可以计算得到这些预测的精确度等指标。训练过程往往需要在训练集上达到一定程度的精确度，不欠拟合或过拟合。监督学习一般解决的问题是分类和回归，代表算法有逻辑斯底回归（Logistic Regression）和神经网络后向传播算法（Back Propagation Neural Network）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;无监督学习（Unsupervised Learning）：输入数据没有任何标记，通过推理数据中已有的结构来构建模型。一般解决的问题是规则学习和聚类，代表算法有Apriori算法和k-means算法。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;半监督学习（Semi-Supervised Learning）：输入数据是标注数据和非标注数据的混合，它也是为了解决预测问题的，但是模型必须同时兼顾学习数据中已经存在的结构和作出预测，即上述监督学习和无监督学习的融合。该方法要解决的问题仍然是分类的回归，代表算法一般是在监督学习的算法上进行扩展，使之可以对未标注数据建模。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;增强学习（Reinforcement Learning）：在这种学习方式中，模型先被构建，然后输入数据刺激模型，输入数据往往来自于环境中，模型得到的结果称之为反馈，使用反馈对模型进行调整。它与监督学习的区别在于反馈数据更多的来自于环境的反馈而不是由人指定。该方式解决的问题是系统与机器人控制，代表算法是Q-学习（Q-learning）和时序差分算法（Temporal difference learning）。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在商业决策中，一般会使用的方法是监督学习和无监督学习。当下一个热门的话题是半监督学习，比如在图片分类中，有很多数据集都是有少量的标记数据和大量的非标记数据。增强学习更多的用于机器人控制机其他的控制系统中。&lt;/p&gt;

&lt;h3 id=&#34;算法相似度-algorithm-similarity:a026595abc6cafd88e910b141621a264&#34;&gt;算法相似度（Algorithm Similarity）&lt;/h3&gt;

&lt;p&gt;一般会根据模型的模式或者函数模式的相似度来对算法进行划分。比如基于树的方法（tree-based method）与神经网络算法（neural network）。当然，这种方法并不完美，因为很多算法可以很容易的被划分到多个类别中去，比如学习矢量量化算法（Learning Vector Quantization）既是神经网络算法也是基于样例的算法（Instance-based method）。在本文中，可以看到很多不同的分类方法。&lt;/p&gt;

&lt;h4 id=&#34;回归-regression:a026595abc6cafd88e910b141621a264&#34;&gt;回归（Regression）&lt;/h4&gt;

&lt;p&gt;回归是在自变量和需要预测的变量之间构建一个模型，并使用迭代的方法逐渐降低预测值和真实值之间的误差。回归方法是统计机器学习的一种
常用的回归算法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ordinary Least Squares（最小二乘法）&lt;/li&gt;
&lt;li&gt;Logistic Regression（逻辑斯底回归）&lt;/li&gt;
&lt;li&gt;Stepwise Regression（逐步回归）&lt;/li&gt;
&lt;li&gt;Multivariate Adaptive Regression Splines（多元自适应回归样条法）&lt;/li&gt;
&lt;li&gt;Locally Estimated Scatterplot Smoothing（局部加权散点平滑法）&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;基于样例的方法-instance-based-methods:a026595abc6cafd88e910b141621a264&#34;&gt;基于样例的方法（Instance-based Methods）&lt;/h4&gt;

&lt;p&gt;基于样例的方法需要一个样本库，当新样本出现时，在样本库中找到最佳匹配的若干个样本，然后做出推测。基于样例的方法又被成为胜者为王的方法和基于内存的学习，该算法主要关注样本之间相似度的计算方法和存储数据的表示形式。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k-Nearest Neighbour (kNN)&lt;/li&gt;
&lt;li&gt;Learning Vector Quantization (LVQ)&lt;/li&gt;
&lt;li&gt;Self-Organizing Map (SOM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;正则化方法-regularization-methods:a026595abc6cafd88e910b141621a264&#34;&gt;正则化方法（Regularization Methods）&lt;/h4&gt;

&lt;p&gt;这是一个对其他方法的延伸（通常是回归方法），这个延伸就是在模型上加上了一个惩罚项，相当于奥卡姆提到，对越简单的模型越有利，有防止过拟合的作用，并且更擅长归纳。我在这里列出它是因为它的流行和强大。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ridge Regression&lt;/li&gt;
&lt;li&gt;Least Absolute Shrinkage and Selection Operator (LASSO)&lt;/li&gt;
&lt;li&gt;Elastic Net&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;决策树模型-decision-tree-learning:a026595abc6cafd88e910b141621a264&#34;&gt;决策树模型（Decision Tree Learning）&lt;/h4&gt;

&lt;p&gt;决策树方法建立了一个根据数据中属性的实际值决策的模型。决策树用来解决归纳和回归问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classification and Regression Tree (CART)&lt;/li&gt;
&lt;li&gt;Iterative Dichotomiser 3 (ID3)&lt;/li&gt;
&lt;li&gt;C4.5&lt;/li&gt;
&lt;li&gt;Chi-squared Automatic Interaction Detection (CHAID)&lt;/li&gt;
&lt;li&gt;Decision Stump&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;li&gt;Multivariate Adaptive Regression Splines (MARS)&lt;/li&gt;
&lt;li&gt;Gradient Boosting Machines (GBM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;贝叶斯-bayesian:a026595abc6cafd88e910b141621a264&#34;&gt;贝叶斯（Bayesian）&lt;/h4&gt;

&lt;p&gt;贝叶斯方法是在解决归类和回归问题中应用了贝叶斯定理的方法。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;Averaged One-Dependence Estimators (AODE)&lt;/li&gt;
&lt;li&gt;Bayesian Belief Network (BBN)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;核方法-kernel-methods:a026595abc6cafd88e910b141621a264&#34;&gt;核方法（Kernel Methods）&lt;/h4&gt;

&lt;p&gt;核方法中最有名的是Support Vector Machines(支持向量机)。这种方法把输入数据映射到更高维度上，将其变得可分，使得归类和回归问题更容易建模。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support Vector Machines (SVM)&lt;/li&gt;
&lt;li&gt;Radial Basis Function (RBF)&lt;/li&gt;
&lt;li&gt;Linear Discriminate Analysis (LDA)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;聚类-clustering-methods:a026595abc6cafd88e910b141621a264&#34;&gt;聚类（Clustering Methods）&lt;/h4&gt;

&lt;p&gt;聚类本身就形容了问题和方法。聚类方法通常是由建模方式分类的比如基于中心的聚类和层次聚类。所有的聚类方法都是利用数据的内在结构来组织数据，使得每组内的点有最大的共同性。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;K-Means&lt;/li&gt;
&lt;li&gt;Expectation Maximisation (EM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;联合规则学习-association-rule-learning:a026595abc6cafd88e910b141621a264&#34;&gt;联合规则学习（Association Rule Learning）&lt;/h4&gt;

&lt;p&gt;联合规则学习是用来对数据间提取规律的方法，通过这些规律可以发现巨量多维空间数据之间的联系，而这些重要的联系可以被组织拿来使用或者盈利。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Apriori algorithm&lt;/li&gt;
&lt;li&gt;Eclat algorithm&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;人工神经网络-artificial-neural-networks:a026595abc6cafd88e910b141621a264&#34;&gt;人工神经网络（Artificial Neural Networks）&lt;/h4&gt;

&lt;p&gt;受生物神经网络的结构和功能的启发诞生的人工神经网络属于模式匹配一类，经常被用于回归和分类问题，但是它存在上百个算法和变种组成。其中有一些是经典流行的算法（深度学习拿出来单独讲）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Perceptron&lt;/li&gt;
&lt;li&gt;Back-Propagation&lt;/li&gt;
&lt;li&gt;Hopfield Network&lt;/li&gt;
&lt;li&gt;Self-Organizing Map (SOM)&lt;/li&gt;
&lt;li&gt;Learning Vector Quantization (LVQ)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;深度学习-deep-learning:a026595abc6cafd88e910b141621a264&#34;&gt;深度学习（Deep Learning）&lt;/h4&gt;

&lt;p&gt;Deep Learning(深度学习)方法是人工神经网络在当下的一个变种。相比传统的神经网络，它更关注更加复杂的网络构成，许多方法都是关心半监督学习，就是一个大数据集中只有少量标注数据的那种问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Restricted Boltzmann Machine (RBM)&lt;/li&gt;
&lt;li&gt;Deep Belief Networks (DBN)&lt;/li&gt;
&lt;li&gt;Convolutional Network&lt;/li&gt;
&lt;li&gt;Stacked Auto-encoders&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;降维-dimensionality-reduction:a026595abc6cafd88e910b141621a264&#34;&gt;降维（Dimensionality Reduction）&lt;/h4&gt;

&lt;p&gt;与聚类方法类似，对数据中的固有结构进行利用，使用无监督的方法学习一种方式，该方式用更少的信息来对数据做归纳和描述。这对于对数据进行可视化或者简化数据很有用，也有去除噪声的影响，经常采用这种方法使得算法更加高效。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Principal Component Analysis (PCA)&lt;/li&gt;
&lt;li&gt;Partial Least Squares Regression (PLS)&lt;/li&gt;
&lt;li&gt;Sammon Mapping&lt;/li&gt;
&lt;li&gt;Multidimensional Scaling (MDS)&lt;/li&gt;
&lt;li&gt;Projection Pursuit&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;组合方法-ensemble-methods:a026595abc6cafd88e910b141621a264&#34;&gt;组合方法（Ensemble Methods）&lt;/h4&gt;

&lt;p&gt;Ensemble methods(组合方法)由许多小的模型组成，这些模型经过独立训练，做出独立的结论，最后汇总起来形成最后的预测。组合方法的研究点集中在使用什么模型以及这些模型怎么被组合起来。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Boosting&lt;/li&gt;
&lt;li&gt;Bootstrapped Aggregation (Bagging)&lt;/li&gt;
&lt;li&gt;AdaBoost&lt;/li&gt;
&lt;li&gt;Stacked Generalization (blending)&lt;/li&gt;
&lt;li&gt;Gradient Boosting Machines (GBM)&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/stdcoutzyx/article/details/44501797&#34;&gt;参考&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>